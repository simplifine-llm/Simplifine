{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Installation\n",
    "To get started with fine-tuning your own LLM models using the Simplifine library, install it directly from the GitHub repository using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the latest Simplifine library from the GitHub repository\n",
    "!pip install git+https://github.com/simplifine-llm/Simplifine.git -q\n",
    "\n",
    "# The 'pip install' command is used to install Python packages.\n",
    "# The '-q' option stands for 'quiet', which minimizes the amount of output produced during the installation.\n",
    "# 'git+https://github.com/simplifine-llm/Simplifine.git' specifies the URL of the GitHub repository from which to install the package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Fine-Tuning LLaMA-3 8B Model\n",
    "\n",
    "In this section, we will focus on fine-tuning the LLaMA-3 8B model using the Simplifine library. Follow the steps below to set up your environment, initialize WandB, prepare your dataset, and configure the Simplifine client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplifine_alpha.train_utils import Client\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Disabling WandB logging. Change this if you'd like to enable it.\n",
    "# Note that you will need a WandB token if you enable logging.\n",
    "wandb.init(mode='disabled')\n",
    "\n",
    "# Define your dataset template and response keys.\n",
    "# Be sure to adjust the keys, response template, and dataset accordingly.\n",
    "template = '''### TITLE: {title}\\n ### ABSTRACT: {abstract}\\n ###EXPLANATION: {explanation}'''\n",
    "response_template = '\\n ###EXPLANATION:'\n",
    "keys = ['title', 'abstract', 'explanation']\n",
    "dataset_name = ''  # Provide a Hugging Face dataset name if applicable.\n",
    "\n",
    "# Set the model name to LLaMA-3 8B. Note that larger models may cause OOM (Out of Memory) errors.\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "hf_token = ''  # Insert your Hugging Face token here to access the LLaMA-3 model.\n",
    "\n",
    "from_hf = True  # Set to False if using custom data.\n",
    "\n",
    "# Option to use your own dataset. Change `own_data` to True if you have custom data.\n",
    "own_data = False\n",
    "if own_data:\n",
    "    from_hf = False\n",
    "    data = {}  # Insert your custom dataset here.\n",
    "\n",
    "# Set up the Simplifine client with your API key and GPU type.\n",
    "simplifine_api_key = ''\n",
    "gpu_type = 'a100'  # Options are 'l4' or 'a100'\n",
    "\n",
    "client = Client(api_key=simplifine_api_key, gpu_type=gpu_type)\n",
    "\n",
    "# Start the training process for fine-tuning LLaMA-3 8B. Adjust parameters for parallelization if needed.\n",
    "client.sft_train_cloud(\n",
    "    model_name=model_name, \n",
    "    from_hf=from_hf, \n",
    "    dataset_name=dataset_name,\n",
    "    keys=keys,\n",
    "    template=template, \n",
    "    job_name='ddp_job',\n",
    "    response_template=response_template, \n",
    "    use_zero=True, \n",
    "    use_ddp=False, \n",
    "    hf_token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Checking Job Status\n",
    "\n",
    "After initiating the fine-tuning process, you might want to check the status of your training jobs. The following code will help you extract and display the statuses of the most recent jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the status of all jobs from the client.\n",
    "status = client.get_all_jobs()\n",
    "\n",
    "# Display the status of the last 5 jobs.\n",
    "for num, i in enumerate(status[-5:]):\n",
    "    print(f'Number {num} status: {i}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Downloading the Trained Model\n",
    "\n",
    "Once your fine-tuning job is complete, the next step is to download the trained model. Follow the steps below to create a folder and save the model locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id = ''  # Get your job ID from the list of job statuses above.\n",
    "\n",
    "# Create a folder to store the trained model.\n",
    "os.mkdir('sf_trained_model_ZeRO')\n",
    "\n",
    "# Download and save the model to the specified folder.\n",
    "# This might take some time, so relax and enjoy a cup of coffee! :)\n",
    "client.download_model(job_id=job_id, extract_to='/content/sf_trained_model_ZeRO')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Testing Your Fine-Tuned Model\n",
    "\n",
    "Now that you've downloaded your fine-tuned model, it's time to test it. We'll load the model and tokenizer using the `transformers` library and generate a sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the path where the trained model is stored.\n",
    "path = '/content/sf_trained_model_ZeRO'\n",
    "\n",
    "# Load the fine-tuned model and tokenizer.\n",
    "sf_model = AutoModelForCausalLM.from_pretrained(path)\n",
    "sf_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "# Create an example input for the model.\n",
    "input_example = '''### TITLE: title 1\\n ### ABSTRACT: abstract 1\\n ###EXPLANATION: '''\n",
    "\n",
    "# Tokenize the input example.\n",
    "input_example = sf_tokenizer(input_example, return_tensors='pt')\n",
    "\n",
    "# Generate output from the fine-tuned model.\n",
    "output = sf_model.generate(input_example['input_ids'],\n",
    "                           attention_mask=input_example['attention_mask'],\n",
    "                           max_length=30,\n",
    "                           eos_token_id=sf_tokenizer.eos_token_id,\n",
    "                           early_stopping=True,\n",
    "                           pad_token_id=sf_tokenizer.eos_token_id)\n",
    "\n",
    "# Decode and print the generated output.\n",
    "print(sf_tokenizer.decode(output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplifine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
